{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwe04E9OhQBf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgAVJvashUnV"
      },
      "outputs": [],
      "source": [
        "# !pip install sentencepiece\n",
        "!pip install transformers==4.17 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8S5a2PxhXNu"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    T5Tokenizer,\n",
        "    TFT5ForConditionalGeneration,\n",
        "    PegasusTokenizer,\n",
        "    TFPegasusForConditionalGeneration,\n",
        "    logging,\n",
        "    T5ForConditionalGeneration\n",
        ")\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "import pickle\n",
        "\n",
        "drive = '/content/drive/MyDrive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9QV3eQthZTu"
      },
      "outputs": [],
      "source": [
        "with open(f'{drive}/train_test_split.pkl', 'rb') as file:\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRzDcXcLhcwN"
      },
      "outputs": [],
      "source": [
        "def generate_summaries(model, tokenizer, X, y, batch_size, max_length=1024, max_length_output=150, min_length=0, num_beams=4):\n",
        "  all_data = []\n",
        "  # Use subset of the data\n",
        "  for i in range(0, len(X), batch_size):\n",
        "      batch = X[i:i+batch_size]\n",
        "\n",
        "      # Keep the labels synchronized\n",
        "      batch_labels = y[i:i+batch_size]\n",
        "\n",
        "      # Generate Inputs. Keep max_length to 1024, since the input text is large.\n",
        "      inputs = tokenizer(batch, max_length=max_length, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "      # Generate Summary IDs\n",
        "      summary_ids = model.generate(\n",
        "          inputs['input_ids'],\n",
        "          attention_mask=inputs['attention_mask'],\n",
        "          max_length=max_length,\n",
        "          min_length=min_length,\n",
        "          max_new_tokens=1024,\n",
        "          num_beams=num_beams,\n",
        "          early_stopping=True\n",
        "      )\n",
        "\n",
        "      # Decode summaries\n",
        "      summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "\n",
        "      # Store inputs, summary IDs, decoded summaries, and original labels in a synchronized manner\n",
        "      for input_text, input_ids, attention_mask, summary_id, summary, label in zip(batch, inputs['input_ids'], inputs['attention_mask'], summary_ids, summaries, batch_labels):\n",
        "          all_data.append({\n",
        "              \"input_text\": input_text,\n",
        "              \"input_ids\": input_ids.numpy().tolist(),\n",
        "              \"attention_mask\": attention_mask.numpy().tolist(),\n",
        "              \"summary_ids\": summary_id.numpy().tolist(),\n",
        "              \"summaries\": summary,\n",
        "              \"label\": label\n",
        "          })\n",
        "\n",
        "  return all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t84JqdXg6s5D"
      },
      "outputs": [],
      "source": [
        "# For full models only, since they were built with a different tensorflow version.\n",
        "!pip install tensorflow==2.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVMzo0wBjBx8"
      },
      "outputs": [],
      "source": [
        "# Generate Summary IDs\n",
        "model_type = 't5'\n",
        "# model_type = 'pegasus'\n",
        "\n",
        "# model_version = \"base\"\n",
        "# model_version = '10k'\n",
        "# model_version = '100k'\n",
        "model_version = 'full'\n",
        "# model_version = '20Epochs'\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "\n",
        "if model_type == \"t5\":\n",
        "  t5_model_name = 't5-base'\n",
        "  tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
        "\n",
        "  if model_version == \"10k\":\n",
        "    model = TFT5ForConditionalGeneration.from_pretrained(f'{drive}/Training_10k/t5_fine_tuned_10k')\n",
        "  elif model_version == \"100k\":\n",
        "    model = TFT5ForConditionalGeneration.from_pretrained(f'{drive}/Training_100k/t5_fine_tuned_100k')\n",
        "  elif model_version == \"base\":\n",
        "    model = TFT5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
        "  elif model_version == \"full\":\n",
        "    model = TFT5ForConditionalGeneration.from_pretrained(f'{drive}/Training_full/t5_fine_tuned')\n",
        "  elif model_version == \"20Epochs\":\n",
        "    model = TFT5ForConditionalGeneration.from_pretrained(f'{drive}/Training_20Epochs/t5_fine_tuned_20Epochs')\n",
        "\n",
        "elif model_type == \"pegasus\":\n",
        "  pegasus_model_name = 'google/pegasus-xsum'\n",
        "  tokenizer = PegasusTokenizer.from_pretrained(pegasus_model_name)\n",
        "  if model_version == \"10k\":\n",
        "    model = TFPegasusForConditionalGeneration.from_pretrained(f'{drive}/Training_10k/pegasus_fine_tuned_10k')\n",
        "  elif model_version == \"100k\":\n",
        "    model = TFPegasusForConditionalGeneration.from_pretrained(f'{drive}/Training_100k/pegasus_fine_tuned_100k')\n",
        "  elif model_version == \"base\":\n",
        "    model = TFPegasusForConditionalGeneration.from_pretrained(pegasus_model_name)\n",
        "  elif model_version == \"full\":\n",
        "    model = TFPegasusForConditionalGeneration.from_pretrained(f'{drive}/Training_full/pegasus_fine_tuned')\n",
        "  elif model_version == \"20Epochs\":\n",
        "    model = TFPegasusForConditionalGeneration.from_pretrained(f'{drive}/Training_20Epochs/pegasus_fine_tuned_20Epochs')\n",
        "\n",
        "print(f\"Tokenizer = {tokenizer}\")\n",
        "print(f\"Model = {model}\")\n",
        "print(f\"Model Version = {model_version}\")\n",
        "print(f\"Model Type = {model_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQh9B3xihpdD"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "subset_size = 500\n",
        "\n",
        "inputs = X_test[:subset_size]\n",
        "\n",
        "# Add prefix for T5 models.\n",
        "if model_type == \"t5\":\n",
        "  prefix = \"summarize: \"\n",
        "  inputs = [f\"{prefix}{content}\" for content in inputs]\n",
        "\n",
        "summaries = generate_summaries(model, tokenizer, inputs, y_test, batch_size)\n",
        "filename = f'{drive}/Summaries/{model_type}_fine_tuned_{model_version}_summaries_{subset_size}.pkl'\n",
        "\n",
        "if model_version == 'base':\n",
        "  filename = f'{drive}/Summaries/{model_type}_{model_version}_summaries_{subset_size}_att.pkl'\n",
        "\n",
        "with open(f'{filename}', 'wb') as f:\n",
        "    pickle.dump(summaries, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}