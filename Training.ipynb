{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PA2vNnsmmP3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gddSNgZulNrg"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install pydot --quiet\n",
        "!pip install gensim --quiet\n",
        "# !pip install sentencepiece\n",
        "\n",
        "!pip install tensorflow==2.15.0 --quiet\n",
        "!pip install tf_keras==2.15.0 --quiet\n",
        "!pip install tensorflow-text==2.15.0 --quiet\n",
        "\n",
        "\n",
        "!pip install transformers==4.17 --quiet\n",
        "\n",
        "!pip install -q evaluate\n",
        "!pip install -q rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7XFwCq7lVi_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import transformers\n",
        "\n",
        "from transformers import (\n",
        "    T5Tokenizer,\n",
        "    TFT5ForConditionalGeneration,\n",
        "    PegasusTokenizer,\n",
        "    TFPegasusForConditionalGeneration,\n",
        ")\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "import sklearn as sk\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from datasets import load_dataset, load_from_disk\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "import pickle\n",
        "\n",
        "drive = '/content/drive/MyDrive/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5CrCKYblds8"
      },
      "outputs": [],
      "source": [
        "def print_version(library_name):\n",
        "    try:\n",
        "        lib = __import__(library_name)\n",
        "        version = getattr(lib, '__version__', 'Version number not found')\n",
        "        print(f\"{library_name} version: {version}\")\n",
        "    except ImportError:\n",
        "        print(f\"{library_name} not installed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdkqaUjpleRb"
      },
      "outputs": [],
      "source": [
        "#confirm versions\n",
        "print_version('numpy')\n",
        "print_version('transformers')\n",
        "print_version('tensorflow')\n",
        "print_version('keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR9VxdTRmD8z"
      },
      "outputs": [],
      "source": [
        "# 95th Percentile. See tokenizing_analysis.ipynb.\n",
        "T5_TRAIN_INPUT_95TH_PERCENTILE = 1046\n",
        "T5_TRAIN_TARGET_95TH_PERCENTILE = 104\n",
        "\n",
        "T5_VAL_INPUT_95TH_PERCENTILE = 1044\n",
        "T5_VAL_TARGET_95TH_PERCENTILE = 103\n",
        "\n",
        "T5_TEST_INPUT_95TH_PERCENTILE = 1044\n",
        "T5_TEST_TARGET_95TH_PERCENTILE = 103\n",
        "\n",
        "PEGASUS_TRAIN_INPUT_95TH_PERCENTILE = 945\n",
        "PEGASUS_TRAIN_TARGET_95TH_PERCENTILE = 93\n",
        "\n",
        "PEGASUS_VAL_INPUT_95TH_PERCENTILE = 943\n",
        "PEGASUS_VAL_TARGET_95TH_PERCENTILE = 93\n",
        "\n",
        "PEGASUS_TEST_INPUT_95TH_PERCENTILE = 944\n",
        "PEGASUS_TEST_TARGET_95TH_PERCENTILE = 92"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSuD4W1dlgsE"
      },
      "outputs": [],
      "source": [
        "# We downloaded this already. Don't need to load this again.\n",
        "# tldr_dataset = load_dataset(\"webis/tldr-17\", trust_remote_code=True, split='train')\n",
        "\n",
        "# tldr_train_dataset, tldr_test_dataset = load_dataset(\"webis/tldr-17\", trust_remote_code=True, split=['train[:70%]', 'train[70%:]'])\n",
        "# tldr_dataset.save_to_disk(f'/{drive}/tldr_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SjlludbHTi0"
      },
      "outputs": [],
      "source": [
        "# We already pickled our test split. Don't need to load this again.\n",
        "# tldr_dataset_local = load_from_disk(f'{drive}/tldr_dataset')\n",
        "# len(tldr_dataset_local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U6chuN-E1Vx"
      },
      "outputs": [],
      "source": [
        "# We already pickled our test split. Don't need to do this again.\n",
        "# X = tldr_dataset_local['content']\n",
        "# y = tldr_dataset_local['summary']\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# NOTE: This is the same size as the test set size.\n",
        "#   0.25 of the size to split * 0.8 of the original data = 0.2.\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "\n",
        "# print(f\"Training set size: {len(X_train)}\")\n",
        "# print(f\"Validation set size: {len(X_val)}\")\n",
        "# print(f\"Test set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHLOR3YJDrxh"
      },
      "outputs": [],
      "source": [
        "# We downloaded this already.\n",
        "# with open(f'{drive}/train_test_split.pkl', 'wb') as file:\n",
        "    # pickle.dump((X_train, X_val, X_test, y_train, y_val, y_test), file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIZ2td_lDylV"
      },
      "outputs": [],
      "source": [
        "with open(f'{drive}/train_test_split.pkl', 'rb') as file:\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pkjdHky-N-s"
      },
      "outputs": [],
      "source": [
        "print(f\"Training set size: {len(X_train)}, {len(y_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}, {len(y_val)}\")\n",
        "print(f\"Test set size: {len(X_test)}, {len(y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWOnClK9t_O1"
      },
      "outputs": [],
      "source": [
        "# Comment out whichever model we don't want to test at the moment.\n",
        "\n",
        "# Load the T5 model and tokenizer.\n",
        "t5_model_name = 't5-base'\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
        "t5_model = TFT5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
        "\n",
        "# Load the PEGASUS model and tokenizer.\n",
        "pegasus_model_name = 'google/pegasus-xsum'\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_model_name)\n",
        "pegasus_model = TFPegasusForConditionalGeneration.from_pretrained(pegasus_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS8HSBlz_8xx"
      },
      "outputs": [],
      "source": [
        "# Experimental wrapper model. This was used for experimentation only, and not in the final\n",
        "\n",
        "def build_t5_training_wrapper_model(t5_model, learning_rate=5e-5, max_length_input=T5_TRAIN_INPUT_95TH_PERCENTILE, max_length_output=T5_TRAIN_TARGET_95TH_PERCENTILE):\n",
        "    input_ids = layers.Input(shape=(max_length_input), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = layers.Input(shape=(max_length_input), dtype=tf.int32, name='attention_mask')\n",
        "    decoder_input_ids = layers.Input(shape=(max_length_output), dtype=tf.int32, name='labels')\n",
        "\n",
        "    t5_logits = t5_model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)[0]\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask, decoder_input_ids],\n",
        "                                  outputs=[t5_logits], name=\"Output\")\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_pegasus_training_wrapper_model(pegasus_model, learning_rate=5e-5, max_length_input=PEGASUS_TRAIN_INPUT_95TH_PERCENTILE, max_length_output=PEGASUS_TRAIN_TARGET_95TH_PERCENTILE):\n",
        "    input_ids = layers.Input(shape=(max_length_input), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = layers.Input(shape=(max_length_input), dtype=tf.int32, name='attention_mask')\n",
        "    decoder_input_ids = layers.Input(shape=(max_length_output), dtype=tf.int32, name='labels')\n",
        "\n",
        "    pegasus_logits = pegasus_model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)[0]\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask, decoder_input_ids],\n",
        "                                  outputs=[pegasus_logits], name=\"Output\")\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQOH_1Pgbcvj"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(text_pairs, tokenizer, model, max_length, max_length_output, is_t5=True):\n",
        "    orig_text = [orig for orig, target in text_pairs]\n",
        "    orig_encoded = tokenizer.batch_encode_plus(\n",
        "        orig_text,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    orig_input_ids = np.array(orig_encoded[\"input_ids\"], dtype=\"int32\")\n",
        "    orig_attention_masks = np.array(orig_encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "\n",
        "    target_text = [target for orig, target in text_pairs]\n",
        "    target_encoded = tokenizer.batch_encode_plus(\n",
        "        target_text,\n",
        "        max_length=max_length_output,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "    label_ids = np.array(target_encoded['input_ids'])\n",
        "\n",
        "    preprocessed_data = [orig_input_ids, orig_attention_masks]\n",
        "    decoder_input_ids = []\n",
        "\n",
        "    # Appropriately handle decoder_input_ids.\n",
        "    if is_t5:\n",
        "      decoder_input_ids = model._shift_right(label_ids)\n",
        "\n",
        "    # Assume this is PEGASUS.\n",
        "    else:\n",
        "      decoder_input_ids = tf.concat([tf.fill([label_ids.shape[0], 1], tokenizer.pad_token_id), label_ids[:, :-1]], axis=-1)\n",
        "\n",
        "    preprocessed_data.append(decoder_input_ids)\n",
        "\n",
        "    return preprocessed_data, label_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO-eK71O9NZ_"
      },
      "outputs": [],
      "source": [
        "prefix = \"summarize: \"\n",
        "\n",
        "# If we uncomment this, add [:subset_size] to each train, val, test split.\n",
        "# subset_size = 10000\n",
        "\n",
        "# Uncomment whichever model we want to test.\n",
        "\n",
        "# NOTE: We already pickled this because this takes a very long time. See later cells.\n",
        "t5_train_input_prefixes = [f\"{prefix}{content}\" for content in  X_train]\n",
        "t5_val_input_prefixes = [f\"{prefix}{content}\" for content in X_val]\n",
        "t5_test_input_prefixes = [f\"{prefix}{content}\" for content in X_test]\n",
        "\n",
        "# T5 - Train\n",
        "t5_train_inputs, t5_target_input_ids = preprocess_data(\n",
        "    list(zip(t5_train_input_prefixes, y_train)),\n",
        "    t5_tokenizer,\n",
        "    t5_model,\n",
        "    max_length=T5_TRAIN_INPUT_95TH_PERCENTILE,\n",
        "    max_length_output=T5_TRAIN_TARGET_95TH_PERCENTILE,\n",
        ")\n",
        "\n",
        "# Pickle for faster use later. We already did this.\n",
        "# t5_train_inputs_path = \"tldr_preprocessed_data/t5_train_inputs.pkl\"\n",
        "# t5_train_target_path = \"tldr_preprocessed_data/t5_target_input_ids.pkl\"\n",
        "\n",
        "# with open(t5_train_inputs_path, 'wb') as f:\n",
        "#     pickle.dump(t5_train_inputs, f)\n",
        "\n",
        "# with open(t5_train_target_path, 'wb') as f:\n",
        "#     pickle.dump(t5_target_input_ids, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqIceHqjWAnI"
      },
      "outputs": [],
      "source": [
        "# PEGASUS - Train\n",
        "\n",
        "# NOTE: We already pickled this because this takes a very long time. See later cells.\n",
        "pegasus_train_inputs, pegasus_target_input_ids = preprocess_data(\n",
        "    list(zip(X_train, y_train)),\n",
        "    pegasus_tokenizer,\n",
        "    pegasus_model,\n",
        "    is_t5=False,\n",
        "    max_length=PEGASUS_TRAIN_INPUT_95TH_PERCENTILE,\n",
        "    max_length_output=PEGASUS_TRAIN_TARGET_95TH_PERCENTILE,\n",
        ")\n",
        "\n",
        "# Pickle for faster use later. We already did this.\n",
        "# pegasus_train_inputs_path = \"tldr_preprocessed_data/pegasus_train_inputs.pkl\"\n",
        "# pegasus_train_target_path = \"tldr_preprocessed_data/pegasus_target_input_ids.pkl\"\n",
        "\n",
        "# with open(pegasus_train_inputs_path, 'wb') as f:\n",
        "#     pickle.dump(pegasus_train_inputs, f)\n",
        "\n",
        "# with open(pegasus_train_target_path, 'wb') as f:\n",
        "#     pickle.dump(pegasus_target_input_ids, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8_wJ-awbZ7c"
      },
      "outputs": [],
      "source": [
        "# # T5 - Val\n",
        "\n",
        "# NOTE: We already pickled this because this takes a very long time. See later cells.\n",
        "t5_val_inputs, t5_val_target_input_ids = preprocess_data(\n",
        "    list(zip(t5_val_input_prefixes, y_val)),\n",
        "    t5_tokenizer,\n",
        "    t5_model,\n",
        "    max_length=T5_TRAIN_INPUT_95TH_PERCENTILE,\n",
        "    max_length_output=T5_TRAIN_TARGET_95TH_PERCENTILE,\n",
        ")\n",
        "\n",
        "# Pickle for faster use later. We already did this.\n",
        "# t5_val_inputs_path = \"tldr_preprocessed_data/t5_val_inputs.pkl\"\n",
        "# t5_val_target_path = \"tldr_preprocessed_data/t5_val_target_input_ids.pkl\"\n",
        "\n",
        "# with open(t5_val_inputs_path, 'wb') as f:\n",
        "#     pickle.dump(t5_val_inputs, f)\n",
        "\n",
        "# with open(t5_val_target_path, 'wb') as f:\n",
        "#     pickle.dump(t5_val_target_input_ids, f)\n",
        "\n",
        "\n",
        "# NOTE: If we want to ignore reprocessing the data, comment them out and uncomment this.\n",
        "# with open(t5_train_inputs_path, 'rb') as f:\n",
        "#     t5_train_inputs = pickle.load(f)\n",
        "\n",
        "# with open(t5_train_target_path, 'rb') as f:\n",
        "#     t5_target_input_ids = pickle.load(f)\n",
        "\n",
        "# with open(t5_val_inputs_path, 'rb') as f:\n",
        "#     t5_val_inputs = pickle.load(f)\n",
        "\n",
        "# with open(t5_val_target_path, 'rb') as f:\n",
        "#     t5_val_target_input_ids = pickle.load(f)\n",
        "\n",
        "\n",
        "# Create Datasets for training.\n",
        "t5_train_dataset = {\n",
        "    'input_ids': t5_train_inputs[0],\n",
        "    'attention_mask': t5_train_inputs[1],\n",
        "    # 'decoder_input_ids': t5_train_inputs[2],\n",
        "    'labels': t5_target_input_ids\n",
        "}\n",
        "\n",
        "t5_val_dataset = {\n",
        "    'input_ids': t5_val_inputs[0],\n",
        "    'attention_mask': t5_val_inputs[1],\n",
        "    # 'decoder_input_ids': t5_val_inputs[2],\n",
        "    'labels': t5_val_target_input_ids\n",
        "}\n",
        "\n",
        "print(len(t5_train_dataset['labels']))\n",
        "print(len(t5_val_dataset['labels']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItqvHzJ8WSFf"
      },
      "outputs": [],
      "source": [
        "# PEGASUS - Val\n",
        "pegasus_val_inputs, pegasus_val_target_input_ids = preprocess_data(\n",
        "    list(zip(X_val, y_val)),\n",
        "    pegasus_tokenizer,\n",
        "    pegasus_model,\n",
        "    is_t5=False,\n",
        "    max_length=PEGASUS_TRAIN_INPUT_95TH_PERCENTILE,\n",
        "    max_length_output=PEGASUS_TRAIN_TARGET_95TH_PERCENTILE)\n",
        "\n",
        "# Pickle for faster use later. We already did this.\n",
        "# pegasus_val_inputs_path = \"tldr_preprocessed_data/pegasus_val_inputs.pkl\"\n",
        "# pegasus_val_target_path = \"tldr_preprocessed_data/pegasus_val_target_input_ids.pkl\"\n",
        "\n",
        "\n",
        "# with open(pegasus_val_inputs_path, 'wb') as f:\n",
        "#     pickle.dump(pegasus_val_inputs, f)\n",
        "\n",
        "# with open(pegasus_val_target_path, 'wb') as f:\n",
        "#     pickle.dump(pegasus_val_target_input_ids, f)\n",
        "\n",
        "\n",
        "# NOTE: If we want to ignore reprocessing the data, comment them out and uncomment this.\n",
        "# with open(pegasus_train_inputs_path, 'rb') as f:\n",
        "#     pegasus_train_inputs = pickle.load(f)\n",
        "\n",
        "# with open(pegasus_train_target_path, 'rb') as f:\n",
        "#     pegasus_target_input_ids = pickle.load(f)\n",
        "\n",
        "# with open(pegasus_val_inputs_path, 'rb') as f:\n",
        "#     pegasus_val_inputs = pickle.load(f)\n",
        "\n",
        "# with open(pegasus_val_target_path, 'rb') as f:\n",
        "#     pegasus_val_target_input_ids = pickle.load(f)\n",
        "\n",
        "\n",
        "# Create Datasets for training.\n",
        "pegasus_train_dataset = {\n",
        "    'input_ids': pegasus_train_inputs[0],\n",
        "    'attention_mask': pegasus_train_inputs[1],\n",
        "    # 'decoder_input_ids': pegasus_train_inputs[2],\n",
        "    'labels': pegasus_target_input_ids\n",
        "}\n",
        "\n",
        "pegasus_val_dataset = {\n",
        "    'input_ids': pegasus_val_inputs[0],\n",
        "    'attention_mask': pegasus_val_inputs[1],\n",
        "    # 'decoder_input_ids': pegasus_val_inputs[2],\n",
        "    'labels': pegasus_val_target_input_ids\n",
        "}\n",
        "\n",
        "print(pegasus_train_dataset['labels'])\n",
        "print(pegasus_val_dataset['labels'])\n",
        "print(pegasus_val_dataset['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIInulnIYrSN"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "class TimeHistory(tf.keras.callbacks.Callback):\n",
        "  def on_train_begin(self, logs=None):\n",
        "    self.train_start_time = time.time()\n",
        "    self.epoch_times = []\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "    self.epoch_start_time = time.time()\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    epoch_time = time.time() - self.epoch_start_time\n",
        "    self.epoch_times.append(epoch_time)\n",
        "    print(f\"Epoch {epoch + 1} training time: {epoch_time:.2f} seconds\")\n",
        "\n",
        "  def on_train_end(self, logs=None):\n",
        "    total_training_time = time.time() - self.train_start_time\n",
        "    print(f\"Total training time: {total_training_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh5xwgDbBybK"
      },
      "outputs": [],
      "source": [
        "# Define early stopping callback.\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    verbose=1,\n",
        "    mode='min',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Define T5 model epoch checkpoint callback.\n",
        "t5_checkpoint_dir = f'{drive}/t5_fine_tuned_model_checkpoints/'\n",
        "t5_checkpoint_filepath = t5_checkpoint_dir + 't5_reddit_tldr_weights.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
        "t5_model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=t5_checkpoint_filepath,\n",
        "    save_weights_only=True)\n",
        "\n",
        "# Define PEGASUS model epoch checkpoint callback.\n",
        "pegasus_checkpoint_dir = f'{drive}/pegasus_fine_tuned_model_checkpoints/'\n",
        "pegasus_checkpoint_filepath = pegasus_checkpoint_dir + 'pegasus_reddit_tldr_weights.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
        "pegasus_model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=pegasus_checkpoint_filepath,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx0jsw_xBA-3"
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "batch_size = 8\n",
        "learning_rate = 5e-5\n",
        "\n",
        "# Just to make sure we're always starting from the base.\n",
        "t5_model_name = 't5-base'\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
        "pegasus_model_name = 'google/pegasus-xsum'\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_model_name)\n",
        "pegasus_model = TFPegasusForConditionalGeneration.from_pretrained(pegasus_model_name)\n",
        "\n",
        "# NOTE: Below is the experimental training wrapper model training.\n",
        "\n",
        "# Build the training wrapper model.\n",
        "# t5_model_wrapper = build_t5_training_wrapper_model(t5_model)\n",
        "\n",
        "# Define time callback.\n",
        "t5_time_callback = TimeHistory()\n",
        "\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# t5_model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "# Define custom ROUGE callback. Metric is only available in keras 3. Hardware was incompatible.\n",
        "# After numerous attempts at this, we decided to forego this.\n",
        "# t5_ROUGE_callback = ROUGECallback(\n",
        "#     model=t5_model,\n",
        "#     tokenizer=t5_tokenizer,\n",
        "#     val_data=t5_val_dataset,\n",
        "#     log_dir='./logs'\n",
        "# )\n",
        "\n",
        "# history = t5_model.fit(\n",
        "#     train_dataset,\n",
        "#     validation_data=t5_val_dataset,\n",
        "#     # batch_size=batch_size,\n",
        "#     epochs=epochs,\n",
        "#     verbose=1,\n",
        "#     callbacks=[early_stopping, t5_model_checkpoint_callback, t5_time_callback]\n",
        "# )\n",
        "\n",
        "# history = t5_model_wrapper.fit(\n",
        "#     t5_train_dataset,\n",
        "#     t5_train_dataset['labels'],\n",
        "#     # train_dataset,\n",
        "#     # t5_train_dataset['labels'],\n",
        "#     # validation_data=val_dataset,\n",
        "#     validation_data=t5_val_dataset,\n",
        "#     batch_size=batch_size,\n",
        "#     epochs=epochs,\n",
        "#     verbose=1,\n",
        "#     callbacks=[early_stopping, t5_model_checkpoint_callback, t5_time_callback]\n",
        "# )\n",
        "\n",
        "# # PEGASUS - Change the name t5 here to generic model wrapper.\n",
        "# pegasus_model_wrapper = build_pegasus_training_wrapper_model(pegasus_model)\n",
        "pegasus_time_callback = TimeHistory()\n",
        "\n",
        "# history = pegasus_model_wrapper.fit(\n",
        "#     pegasus_train_dataset,\n",
        "#     pegasus_train_dataset['labels'],\n",
        "#     validation_data=pegasus_val_dataset,\n",
        "#     batch_size=batch_size,\n",
        "#     epochs=epochs,\n",
        "#     verbose=1,\n",
        "#     callbacks=[early_stopping, pegasus_model_checkpoint_callback, pegasus_time_callback]\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1BqwOERzi--t"
      },
      "outputs": [],
      "source": [
        "# NOTE: Below is the model training without the wrapper model. Comment/Uncomment the model info that you need.\n",
        "\n",
        "model = TFPegasusForConditionalGeneration.from_pretrained(pegasus_model_name)\n",
        "# model = TFT5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    pegasus_train_dataset,\n",
        "    pegasus_train_dataset['labels'],\n",
        "    validation_data=pegasus_val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    callbacks=[pegasus_model_checkpoint_callback, pegasus_time_callback]\n",
        ")\n",
        "\n",
        "# history = model.fit(\n",
        "#     t5_train_dataset,\n",
        "#     t5_train_dataset['labels'],\n",
        "#     validation_data=t5_val_dataset,\n",
        "#     batch_size=batch_size,\n",
        "#     epochs=epochs,\n",
        "#     verbose=1,\n",
        "#     callbacks=[t5_model_checkpoint_callback, t5_time_callback]\n",
        "#     # callbacks=[early_stopping, t5_model_checkpoint_callback, t5_time_callback]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XntPMATn9UwD"
      },
      "outputs": [],
      "source": [
        "print(\"Training History:\", history.history)\n",
        "\n",
        "# with open(f'{drive}/Training_10Epochs/t5_training_history.pkl', 'wb') as f:\n",
        "with open(f'{drive}/Training_20Epochs/pegasus_training_history.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBWvmTMJC6_y"
      },
      "outputs": [],
      "source": [
        "model.save_weights(f'{drive}/pegasus_fine_tuned_weights.h5')\n",
        "model.save_pretrained(f'{drive}/pegasus_fine_tuned')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
